{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This notebook aimes to create a spam messages generation model based on several datasets contaning ones.\n#### Potentially, it could be useful in further spam/non-spam classification for class imbalance overcoming or spam messages featues extraction.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-05T07:53:21.772303Z","iopub.execute_input":"2022-08-05T07:53:21.772733Z","iopub.status.idle":"2022-08-05T07:53:21.781790Z","shell.execute_reply.started":"2022-08-05T07:53:21.772698Z","shell.execute_reply":"2022-08-05T07:53:21.780876Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/spam-or-not-spam-dataset/spam_or_not_spam.csv\n/kaggle/input/spam-mails-dataset/spam_ham_dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom tqdm import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download the data","metadata":{}},{"cell_type":"code","source":"\n# train_data1 = pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv', encoding='ISO-8859-1')\n# ref_data1 = train_data1[train_data1['v1'] != 'spam']['v2']\n# train_data1 = train_data1[train_data1['v1'] == 'spam']['v2']\n\ntrain_data2 = pd.read_csv('../input/spam-mails-dataset/spam_ham_dataset.csv')\nref_data2 = train_data2[train_data2['label'] != 'spam']['text']\ntrain_data2 = train_data2[train_data2['label'] == 'spam']['text']\n\ntrain_data3 = pd.read_csv('../input/spam-or-not-spam-dataset/spam_or_not_spam.csv')\nref_data3 = train_data3[train_data3['label'] != 1]['email']\ntrain_data3 = train_data3[train_data3['label'] == 1]['email']\n\ntrain_data = pd.concat([ train_data2, train_data3])\nref_data = pd.concat([ ref_data2, ref_data3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"import re \nimport nltk\nfrom spacy.lang.en import English\nfrom nltk.corpus import stopwords\n\nnlp = English()\n\nfrom functools import reduce\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport re \nfrom wordcloud import WordCloud\n\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check for Nans","metadata":{}},{"cell_type":"code","source":"train_data.isnull().sum(), ref_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop nan\ntrain_data.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Statistics","metadata":{}},{"cell_type":"code","source":"token_data = [t.lower().split() for t in train_data.values]\n\n\nprint(f'Total number of messages: {len(train_data)}')\nprint(f'Total number of tokens: {sum([len(t) for t in token_data])}')\nprint(f'Total number of UNIQUE tokens: {len(set(reduce(lambda x, y: x + y, token_data)))}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Roughly estimated including punctuation and other different kinds of irrelevant characters, we have more than 500k tokens and more than 50k unique tokens","metadata":{}},{"cell_type":"code","source":"# ivestigating text lengths\ntext_lens = [len(t) for t in token_data]\n\nfig, axes = plt.subplots(2, 1, figsize=(15, 7))\n\nsns.boxplot(text_lens, ax=axes[0])\nsns.histplot(text_lens, kde=True, ax=axes[1])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# closer look on text lens\ntext_lens = [len(t) for t in token_data if len(t) < 2000]\n\nfig, axes = plt.subplots(2, 1, figsize=(15, 7))\n\nsns.boxplot(text_lens, ax=axes[0])\nsns.histplot(text_lens, kde=True, ax=axes[1])\naxes[1].axvline(x=np.mean(text_lens), ymax=0.8, c='r')\naxes[1].text(np.mean(text_lens)-70, 615, f'mean = {int(round(np.mean(text_lens), 0))}', c='r')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe many outliers which can exceed nearly 12k tokens in one message. The average length, however, is 169 tokens. 75% of have no more than 186 tokens.","metadata":{}},{"cell_type":"code","source":"# most frequent tokens\ntop_tokens = pd.Series(reduce(lambda x, y: x + y, token_data)).value_counts()[:30]\n\nplt.figure(figsize=(15, 8))\nsns.barplot(y=top_tokens.index, x=top_tokens.values, orient='h')\nplt.title('Most frequent tokens')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_texts_together = reduce(lambda x, y: f'{x}{y}', train_data)\npunctuation_pattern = re.compile(f'[^{string.punctuation}]')\n\npunctuation = re.sub(punctuation_pattern, '', all_texts_together)\n\ntop_punct = pd.Series(list(punctuation)).value_counts()\n\nplt.figure(figsize=(15, 8))\nsns.barplot(y=top_punct.index, x=top_punct.values, orient='h')\nplt.title('Most frequent punctuations')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For further EDA some Preprocesing is required.","metadata":{}},{"cell_type":"code","source":"class EDAPreprocesser:\n    def __init__(self):\n        self.punkt_pattern = re.compile('[^A-z ]')\n        self.tokenizer = nlp.tokenizer\n        self.stemmer = nltk.stem.SnowballStemmer('english')\n        self.stopwords = stopwords.words('english')\n    \n    def delete_punctuation(self, texts):\n        return [re.sub(self.punkt_pattern, '', t) for t in tqdm(texts)]\n    \n    def stem(self, texts):\n        return [[self.stemmer.stem(token) for token in text] for text in tqdm(texts)]\n    \n    def tokenize(self, texts):\n        return [[*map(lambda x: str(x), self.tokenizer(text))] for text in tqdm(texts)]\n    \n    \n    def delete_stopwords(self, texts):\n        return [[t for t in text if t not in self.stopwords] for text in tqdm(texts)]\n    \n    def remove_spaces(self, texts):\n        return [[t for t in text if not t.isspace()] for text in texts]\n    \n    def remove_long_words(self, texts):\n        return [[t for t in text if len(t) < 20] for text in texts]\n    \n    def transform(self, texts):\n        print('Lowering...')\n        texts = [*map(str.lower, texts)] # lower\n        \n        print('Removing punctuation...')\n        texts = self.delete_punctuation(texts)  # delete punktuation\n        \n        print('Tokenization...')\n        texts = self.tokenize(texts)\n        \n        print('Stemming...')\n        texts = self.stem(texts)\n        \n        print('Removing stopwords...')\n        texts = self.delete_stopwords(texts)\n        \n        print('Removing spaces...')\n        texts = self.remove_spaces(texts)\n        \n        print('Removing long words...')\n        texts = self.remove_long_words(texts)\n        \n        return texts\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda_prep = EDAPreprocesser()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts_prep = pd.Series(eda_prep.transform(train_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens_list = pd.Series(reduce(lambda x, y: x + y, texts_prep))\ntop_words = tokens_list.value_counts()[:30]\n\nplt.figure(figsize=(15, 8))\nsns.barplot(y=top_words.index, x=top_words.values, orient='h')\nplt.title('Most frequent tokens')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create and generate a word cloud image:\nwordcloud = WordCloud(background_color='white',\n                     width=800, height=400,\n                     random_state=42,\n                     collocations=False).generate(' '.join(tokens_list))\n\nplt.figure(figsize=(15, 8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bigrams_list = pd.Series([' '.join(bi) for text in  texts_prep for bi in zip(text[:-1], text[1:])])\ntop_bigrams = bigrams_list.value_counts()[:30]\n\nplt.figure(figsize=(15, 8))\nsns.barplot(y=top_bigrams.index, x=top_bigrams.values, orient='h')\nplt.title('Most frequent tokens')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# key words\nref_prep = eda_prep.transform(ref_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ref_list = pd.Series(reduce(lambda x, y: x + y, ref_prep))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freqs_texts = dict(tokens_list.value_counts())\nfreqs_ref = dict(ref_list.value_counts())\nkeyword_score = {token: freq / freqs_ref.get(token, 1) for token, freq in freqs_texts.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_keywords = sorted([(k, v) for k, v in keyword_score.items()], key=lambda x: x[1], reverse=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 8))\nsns.barplot(y=[y[0] for y in top_keywords[:30]], x=[x[1] for x in top_keywords[:30]], orient='h')\nplt.title('Most frequent keywords')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Dataset","metadata":{}},{"cell_type":"code","source":"import torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nDEVICE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Preprocesser:\n    \n    def __init__(self):\n        self.tokenizer = nlp.tokenizer\n        self.chars_pattern = re.compile(f'[^A-z0-9{string.punctuation} ]')\n    \n    def tokenize(self, texts):\n        return [[*map(lambda x: str(x), self.tokenizer(text))] for text in tqdm(texts)]\n    \n    def remove_spaces(self, texts):\n        return [[t for t in text if not t.isspace()] for text in texts]\n    \n    def remove_long_words(self, texts):\n        return [[t for t in text if len(t) < 15] for text in texts]\n    \n    def remove_chars(self, texts):\n        return [re.sub(self.chars_pattern, '', text) for text in tqdm(texts)]\n    \n    def transform(self, texts):\n        print('Lowering...')\n        texts = [*map(str.lower, texts)] # lower\n        \n        print('Removing some characters...')\n        texts = self.remove_chars(texts)\n        \n        print('Tokenization...')\n        texts = self.tokenize(texts)\n        \n        print('Removing spaces...')\n        texts = self.remove_spaces(texts)\n        \n        print('Removing long words...')\n        texts = self.remove_long_words(texts)\n        \n        return texts\n\n    \nclass Indexer:\n    def __init__(self, voc):\n        self.voc = voc\n        self.token_idx = {tok: i for i, tok in enumerate(self.voc)}\n        self.idx_token =  {i: tok for tok, i in self.token_idx.items()}\n        \n    def encode(self, text):\n        return [self.token_idx[tok] for tok in text]\n\n    def decode(self, text):\n        return [self.idx_token[tok] for tok in text]\n\nclass Dataset:\n    def __init__(self, texts, context_size=5, voc=None):\n        self.context_size = context_size\n        self.token_count = self.__count_tokens(texts)\n        self.voc = voc\n        self.voc_len = len(self.voc)\n        \n        self.data = self.__create_data(texts)\n        self.indexer = Indexer(self.voc)\n        self.encoded_data = self.__encode_data()\n        \n    def __encode_data(self):\n        encoded_data = [(self.indexer.encode(text), \n                        self.__ohe_target(target)) for text, target in self.data]\n        return encoded_data\n        \n    def __ohe_target(self, y):\n        #new_y = [0] * self.voc_len\n        #new_y[self.indexer.encode([y])[0]] = 1\n        return self.indexer.encode([y])\n            \n    def __create_data(self, texts):\n        storage = []\n        \n        for text in texts:\n            for start in range(len(text)-(self.context_size+1)):\n                storage.append((text[start:start+self.context_size], \n                                text[start+self.context_size+1]))\n                               \n        return storage\n    \n    def __count_tokens(self, texts):\n        token_count = {}\n        \n        for text in texts:\n            for token in text:\n                token_count[token] = token_count.get(token, 1) + 1\n        return token_count\n    \n    def __getitem__(self, i):\n        return torch.Tensor(self.encoded_data[i][0]).int(), torch.Tensor(self.encoded_data[i][1]).int()\n    \n    def __len__(self):\n        return len(self.data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voc = set(reduce(lambda x, y: x + y, Preprocesser().transform(train_data)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_texts, test_texts = train_test_split(train_data, test_size=0.1, \n                                          shuffle=True,\n                                          random_state=42)\ntrain_texts, val_texts = train_test_split(train_texts, test_size=0.05,\n                                         shuffle=True,\n                                         random_state=42)\n\nds_train = Dataset(Preprocesser().transform(train_texts), voc=voc)\nds_val = Dataset(Preprocesser().transform(val_texts), voc=voc)\nds_test = Dataset(Preprocesser().transform(test_texts),voc=voc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_train.data[0][0], ds_train.indexer.encode(ds_train.data[0][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_train.data[0][0], ds_val.indexer.encode(ds_train.data[0][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"import pytorch_lightning as pl\nfrom torch import optim, nn, utils\nfrom transformers import BertTokenizer, DistilBertModel\nimport torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\nSEQ_LEN = 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextGenerator(pl.LightningModule):\n    def __init__(self, voc_size, num_layers=1, embed_size=300):\n        super().__init__()\n        self.voc_size = voc_size\n        self.num_layers = 1\n        self.embed_size = embed_size\n        self.criterion = nn.CrossEntropyLoss()\n        \n        self.embedding = nn.Embedding(voc_size, embed_size, device=DEVICE)\n\n        self.bilstm = nn.LSTM(input_size=embed_size,\n            hidden_size=embed_size,\n            num_layers=num_layers,\n            dropout=0.2,\n            bidirectional=True)\n        \n        self.lstm = nn.LSTM(input_size=embed_size*2,\n            hidden_size=embed_size,\n            num_layers=num_layers,\n            dropout=0.2,\n            bidirectional=False)\n        \n        self.fc1 = nn.Linear(SEQ_LEN*embed_size, SEQ_LEN*embed_size*2)\n        self.fc2 = nn.Linear(SEQ_LEN*embed_size*2, voc_size)\n        self.relu = nn.ReLU()\n        \n        self.prev_state_bilstm = self.init_state(bi=True)\n        self.prev_state_lstm = self.init_state(bi=False)\n        \n        \n    def forward(self, x):\n        embed =  self.embedding(x)\n        output, prev_state_bilstm = self.bilstm(embed, self.prev_state_bilstm)\n        #output = output.view(SEQ_LEN, BATCH_SIZE, 2, self.embed_size)\n        output, prev_state_lstm = self.lstm(output, self.prev_state_lstm)\n        output = output.view(BATCH_SIZE, SEQ_LEN*self.embed_size)\n\n        output = self.relu(self.fc1(output))\n        logits = self.fc2(output)\n        \n        self.prev_state_bilstm = prev_state_bilstm[0].detach(), prev_state_bilstm[1].detach()\n        self.prev_state_lstm = prev_state_lstm[0].detach(), prev_state_lstm[1].detach()\n        \n        return logits\n    \n    def init_state(self, bi):\n        return (torch.zeros(self.num_layers+bi, SEQ_LEN, self.embed_size),\n                torch.zeros(self.num_layers+bi, SEQ_LEN, self.embed_size))\n    \n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters())\n        return optimizer\n    \n    def training_step(self, batch, batch_idx):\n        X, y = batch\n        y = y.long().to(DEVICE)\n        X.to(DEVICE)\n        \n        output = torch.squeeze(self.forward(X))\n        \n        loss = self.criterion(output,y.squeeze())\n        self.log('train_loss', loss, prog_bar=True)\n        \n        return loss\n        \n    \n    def val_step(self, batch, batch_idx):\n        X, y = batch\n        y = y.long().to(DEVICE)\n        X.to(DEVICE)\n        \n        output = torch.squeeze(self.forward(X))\n        \n        loss = self.criterion(output,y.squeeze())\n        self.log('val_loss', loss, prog_bar=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = utils.data.DataLoader(ds_train, batch_size=BATCH_SIZE)\nval_loader = utils.data.DataLoader(ds_val, batch_size=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"next(iter(train_loader.dataset))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TextGenerator(ds_train.voc_len)\ntrainer = pl.Trainer()\ntrainer.fit(model, train_loader, val_loader,gpus=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]}]}